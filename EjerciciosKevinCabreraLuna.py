# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19aW0aHSEG5Ej39rAUO3w7oGvqMjGu9Gv
"""

!pip install pyspark

# Cree un RDD llamado lenguajes que contenga los siguientes lenguajes de programación: Python, R, C, Scala, Rugby y SQL.
#Obtener un nuevo RDD a partir del RDD lenguajes donde todos los lenguajes de programación estén en mayúsculas.
#Obtener un nuevo RDD a partir del RDD de lenguajes donde todos los lenguajes de programación estén en minúsculas.
#Crear un nuevo RDD que solo contenga aquellos lenguajes de programación que comiencen con la letra R.

from pyspark import SparkContext
sc = SparkContext("local", "ejemplo")
lenguajes = sc.parallelize(["Python", "R", "C", "Scala", "Rugby", "SQL"])
lenguajes_mayusculas = lenguajes.map(lambda x: x.upper())
lenguajes_minusculas = lenguajes.map(lambda x: x.lower())
lenguajes_con_R = lenguajes.filter(lambda x: x.startswith("R"))
print("Lenguajes en mayúsculas:", lenguajes_mayusculas.collect())
print("Lenguajes en minúsculas:", lenguajes_minusculas.collect())
print("Lenguajes que comienzan con 'R':", lenguajes_con_R.collect())
sc.stop()

#Crear un RDD llamado pares que contenga los números pares existentes en el intervalo [20;30]
#Crear un RDD llamado sqrt, este debe contener la raíz cuadrada de los elementos que componene el RDD pares
#Obtener una lista compuesta por los números pares en el intervalo [20;30] y sus respectivas raíces cuadradas. Un ejemplo de resultado deseado para el
#intervalo [50;60] sería la lista [50, 7.071067811..., 52, 7.21110255...] Elevar el número de particiones del RDD sqrt a 20.
#Si tuviera que disminuir el número de particiones luego de haberlo establecido en 20, ¿qué función utilizaría para hacer más eficiente el código?
from pyspark import SparkContext
sc = SparkContext("local", "ejemplo")
pares = sc.parallelize(range(20, 31, 2))
sqrt = pares.map(lambda x: (x, x ** 0.5))
numeros_y_raices = sqrt.collect()
sqrt = sqrt.repartition(20)
sqrt = sqrt.coalesce(10)
print("Números pares en el intervalo [20;30]:", pares.collect())
print("Raíces cuadradas de los números pares:", sqrt.collect())
print("Lista de números pares y sus raíces cuadradas:", numeros_y_raices)
sc.stop()

# Crear un RDD a partir de la información adjunta. Supongamos que el RDD resultante del tipo clave refleja las transacciones realizadas por número de cuentas. Obtener el
#monto total por cada cuenta
#```
#(1001, 52.3)
#(1005, 20.8)
#(1001, 10.1)
#(1004, 52.7)
#(1005, 20.7)
#(1002, 85.3)
#(1004, 20.9)
#```
from pyspark import SparkContext
sc = SparkContext("local", "ejemplo")
transacciones = sc.parallelize([(1001, 52.3), (1005, 20.8), (1001, 10.1), (1004, 52.7), (1005, 20.7), (1002, 85.3), (1004, 20.9)])
monto_total_por_cuenta = transacciones.reduceByKey(lambda x, y: x + y)
print("Monto total por cada cuenta:", monto_total_por_cuenta.collect())
sc.stop()